{
 "cells": [
  {
   "source": [
    "## References:\n",
    "\n",
    "1. https://www.espncricinfo.com\n",
    "2. https://medium.com/swlh/web-scraping-cricinfo-data-c134fce79a33"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IPLDataScrapper:\n",
    "\n",
    "    def __init__(self, year, data_dir='Data'):\n",
    "        self.domain = 'https://www.espncricinfo.com'\n",
    "        self.data_dir = data_dir\n",
    "        if year >= 2010 and year <= 2020:\n",
    "            self.year = year\n",
    "            if self.year >= 2014 and self.year <= 2015:\n",
    "                self.season_url = self.domain + '/scores/series/8048/season/{}/pepsi-indian-premier-league?view=results'.format(self.year)\n",
    "            else:\n",
    "                self.season_url = self.domain + '/scores/series/8048/season/{}/indian-premier-league?view=results'.format(self.year)\n",
    "        else:\n",
    "            raise ValueError(\"Scrapper is defined only for the seasons from 2010 to 2020\")\n",
    "\n",
    "    def __extract_match_urls(self):\n",
    "        season_page = requests.get(self.season_url)\n",
    "        if season_page.status_code == 200:\n",
    "            soup = BeautifulSoup(season_page.content, 'html.parser')\n",
    "            matches = soup.find_all(class_='col-md-8 col-16')\n",
    "            match_urls = []\n",
    "            for match in matches:\n",
    "                match_url = self.domain + match.find('a', href=True)['href']\n",
    "                match_urls.append(match_url)\n",
    "        else:\n",
    "            raise ValueError(\"Response status code: {}\".format(season_page.status_code))\n",
    "\n",
    "        return match_urls\n",
    "\n",
    "    def __extract_batsman_data(self, soup):\n",
    "        batsman_tables = soup.find_all(class_=\"table batsman\")\n",
    "        #assert len(batsman_tables) == 2\n",
    "\n",
    "        columns = ['name', 'wicket', 'runs', 'balls', 'duration', 'fours', 'sixes', 'strike_rate']\n",
    "        for inning, batsman_table in enumerate(batsman_tables, start=1):\n",
    "            rows = batsman_table.find_all('tr')\n",
    "            batsman_list = []\n",
    "            for i in range(1, len(rows), 2):\n",
    "                batsman_row = rows[i]\n",
    "                cells = batsman_row.find_all('td')\n",
    "                cells = [cell.text.strip() for cell in cells]\n",
    "\n",
    "                if cells[0] == 'Extras':\n",
    "                    row = ['Extras', 'Extras', cells[2], '0', '0', '0', '0', '0']\n",
    "                    batsman_list.append(row)\n",
    "                elif len(cells) > 7:\n",
    "                    row = cells\n",
    "                    batsman_list.append(row)\n",
    "                else:\n",
    "                    batsmen = [batsman.strip() for batsman in cells[0][len('Did not bat: '):].split(',')]\n",
    "                    for batsman in batsmen:\n",
    "                        row = [batsman, 'Did not bat', '0', '0', '0', '0', '0', '0']\n",
    "                        batsman_list.append(row)\n",
    "                \n",
    "            batsman_df = pd.DataFrame(batsman_list, columns=columns)\n",
    "            if inning == 1:\n",
    "                batsman_df_1 = batsman_df\n",
    "                batsman_df_1['inning'] = 1\n",
    "            elif inning == 2:  \n",
    "                batsman_df_2 = batsman_df\n",
    "                batsman_df_2['inning'] = 2\n",
    "\n",
    "        if len(batsman_tables) == 2:\n",
    "            batsman_df = pd.concat([batsman_df_1, batsman_df_2])\n",
    "        elif len(batsman_tables) == 1:\n",
    "            batsman_df = batsman_df_1\n",
    "        elif len(batsman_tables) == 0:\n",
    "            batsman_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "        return batsman_df \n",
    "\n",
    "\n",
    "    def __extract_bowler_data(self, soup):\n",
    "        bowler_tables = soup.find_all(class_=\"table bowler\")\n",
    "        #assert len(bowler_tables) == 2\n",
    "\n",
    "        columns = ['name', 'overs', 'maidens', 'runs', 'wickets', 'economy', 'dots', 'fours', 'sixes', 'wides', 'no_balls']\n",
    "        for inning, bowler_table in enumerate(bowler_tables, start=1):\n",
    "            rows = bowler_table.find_all('tr')\n",
    "            bowler_list = []\n",
    "            for i in range(1, len(rows)):\n",
    "                bowler_row = rows[i]\n",
    "                cells = bowler_row.find_all('td')\n",
    "                cells = [cell.text.strip() for cell in cells]\n",
    "                row = cells\n",
    "                bowler_list.append(row)\n",
    "                \n",
    "            bowler_df = pd.DataFrame(bowler_list, columns=columns)\n",
    "            if inning == 1:\n",
    "                bowler_df_1 = bowler_df\n",
    "                bowler_df_1['inning'] = 1\n",
    "            elif inning == 2:  \n",
    "                bowler_df_2 = bowler_df\n",
    "                bowler_df_2['inning'] = 2\n",
    "            \n",
    "\n",
    "        if len(bowler_tables) == 2:\n",
    "            bowler_df = pd.concat([bowler_df_1, bowler_df_2])\n",
    "        elif len(bowler_tables) == 1:\n",
    "            bowler_df = bowler_df_1\n",
    "        elif len(bowler_tables) == 0:\n",
    "            bowler_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "        return bowler_df\n",
    "\n",
    "    def scrape(self):\n",
    "        self.match_urls = self.__extract_match_urls()\n",
    "        self.season_dir = os.path.join(self.data_dir, str(self.year))\n",
    "        if not os.path.exists(self.season_dir):\n",
    "            os.mkdir(self.season_dir)\n",
    "        for match_url in tqdm(self.match_urls, desc=\"Matches\", leave=False):\n",
    "            match_page = requests.get(match_url)\n",
    "            soup = BeautifulSoup(match_page.content, 'html.parser')\n",
    "            match_id, location, date, _ = soup.find(class_='desc text-truncate').get_text().split(',')\n",
    "            match_id = match_id.replace('/', ' and ')\n",
    "            batsman_df = self.__extract_batsman_data(soup)\n",
    "            bowler_df = self.__extract_bowler_data(soup)\n",
    "            match_dir = os.path.join(self.season_dir, match_id)\n",
    "            if not os.path.exists(match_dir):\n",
    "                os.mkdir(match_dir)\n",
    "            batsman_df.to_csv(os.path.join(match_dir, 'batsman_df.csv'), index=False)\n",
    "            bowler_df.to_csv(os.path.join(match_dir, 'bowler_df.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Seasons', max=9.0, style=ProgressStyle(description_width=…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "734bab14d94e444eb0cffcc52511f454"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Matches', max=74.0, style=ProgressStyle(description_width…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1495076938c44397a4c3782378ab5c74"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n"
    },
    {
     "output_type": "error",
     "ename": "UnboundLocalError",
     "evalue": "local variable 'batsman_df' referenced before assignment",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-6ba26fcc8436>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0myear\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2011\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2020\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Seasons'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mscrapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIPLDataScrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myear\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0myear\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mscrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscrape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-08ba21bbd5ea>\u001b[0m in \u001b[0;36mscrape\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0mmatch_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'desc text-truncate'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0mmatch_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' and '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m             \u001b[0mbatsman_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__extract_batsman_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m             \u001b[0mbowler_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__extract_bowler_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mmatch_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseason_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatch_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-08ba21bbd5ea>\u001b[0m in \u001b[0;36m__extract_batsman_data\u001b[0;34m(self, soup)\u001b[0m\n\u001b[1;32m     60\u001b[0m                 \u001b[0mbatsman_df_2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'inning'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatsman_df\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m             \u001b[0mbatsman_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatsman_df_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatsman_df_2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatsman_df\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'batsman_df' referenced before assignment"
     ]
    }
   ],
   "source": [
    "for year in tqdm(range(2011, 2020), 'Seasons'):\n",
    "    scrapper = IPLDataScrapper(year=year)\n",
    "    scrapper.scrape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}